steps:
# 1. Create a Docker image containing flink-bigquery-connector repo.
  - name: 'gcr.io/cloud-builders/docker'
    id: 'docker-build'
    args: ['build', '--tag=gcr.io/$PROJECT_ID/dataproc-flink-bigquery-connector-nightly', '-f', 'cloudbuild/Dockerfile', '.']
#
## 2. Fetch maven and dependencies.
#  - name: 'gcr.io/$PROJECT_ID/dataproc-flink-bigquery-connector-nightly'
#    id: 'init'
#    waitFor: ['docker-build']
#    entrypoint: 'bash'
#    args: ['/workspace/cloudbuild/nightly.sh', 'init']
#    env:
#      - 'GCS_JAR_LOCATION=${_GCS_JAR_LOCATION}'
#      - 'MVN_JAR_LOCATION=${_MVN_JAR_LOCATION}'
#
## 3. Start the simple table read e2e test.
#  - name: 'gcr.io/$PROJECT_ID/dataproc-flink-bigquery-connector-nightly'
#    id: 'e2e-bounded-read-small-table-test'
#    waitFor: ['init']
#    entrypoint: 'bash'
#    args: ['/workspace/cloudbuild/nightly.sh', 'e2e_bounded_read_small_table_test']
#    env:
#      - 'GCS_JAR_LOCATION=${_GCS_JAR_LOCATION}'
#      - 'PROJECT_ID=${_PROJECT_ID}'
#      - 'REGION_SMALL_TEST=${_REGION_SMALL_TEST}'
#      - 'CLUSTER_NAME_SMALL_TEST=${_CLUSTER_NAME_SMALL_TEST}'
#      - 'PROJECT_NAME=${_PROJECT_NAME}'
#      - 'DATASET_NAME=${_DATASET_NAME}'
#      - 'TABLE_NAME_SIMPLE_TABLE=${_TABLE_NAME_SIMPLE_TABLE}'
#      - 'AGG_PROP_NAME_SIMPLE_TABLE=${_AGG_PROP_NAME_SIMPLE_TABLE}'
#
## 4. Start the nested schema table read e2e test.
#  - name: 'gcr.io/$PROJECT_ID/dataproc-flink-bigquery-connector-nightly'
#    id: 'e2e-bounded-read-nested-schema-table-test'
#    waitFor: ['e2e-bounded-read-small-table-test']
#    entrypoint: 'bash'
#    args: ['/workspace/cloudbuild/nightly.sh', 'e2e_bounded_read_nested_schema_table_test']
#    env:
#      - 'GCS_JAR_LOCATION=${_GCS_JAR_LOCATION}'
#      - 'PROJECT_ID=${_PROJECT_ID}'
#      - 'REGION_SMALL_TEST=${_REGION_SMALL_TEST}'
#      - 'CLUSTER_NAME_SMALL_TEST=${_CLUSTER_NAME_SMALL_TEST}'
#      - 'PROJECT_NAME=${_PROJECT_NAME}'
#      - 'DATASET_NAME=${_DATASET_NAME}'
#      - 'TABLE_NAME_COMPLEX_SCHEMA_TABLE=${_TABLE_NAME_COMPLEX_SCHEMA_TABLE}'
#      - 'AGG_PROP_NAME_COMPLEX_SCHEMA_TABLE=${_AGG_PROP_NAME_COMPLEX_SCHEMA_TABLE}'
#
## 5. Start the query read e2e test.
#  - name: 'gcr.io/$PROJECT_ID/dataproc-flink-bigquery-connector-nightly'
#    id: 'e2e-bounded-read-query-test'
#    waitFor: ['e2e-bounded-read-nested-schema-table-test']
#    entrypoint: 'bash'
#    args: ['/workspace/cloudbuild/nightly.sh', 'e2e_bounded_read_query_test']
#    env:
#      - 'GCS_JAR_LOCATION=${_GCS_JAR_LOCATION}'
#      - 'PROJECT_ID=${_PROJECT_ID}'
#      - 'REGION_SMALL_TEST=${_REGION_SMALL_TEST}'
#      - 'CLUSTER_NAME_SMALL_TEST=${_CLUSTER_NAME_SMALL_TEST}'
#      - 'PROJECT_NAME=${_PROJECT_NAME}'
#      - 'DATASET_NAME=${_DATASET_NAME}'
#      - 'QUERY=${_QUERY}'
#
## 6. Start the large table read e2e test.
#  - name: 'gcr.io/$PROJECT_ID/dataproc-flink-bigquery-connector-nightly'
#    id: 'e2e-bounded-read-large-table-test'
#    waitFor: ['e2e-bounded-read-query-test']
#    entrypoint: 'bash'
#    args: ['/workspace/cloudbuild/nightly.sh', 'e2e_bounded_read_large_table_test']
#    env:
#      - 'GCS_JAR_LOCATION=${_GCS_JAR_LOCATION}'
#      - 'PROJECT_ID=${_PROJECT_ID}'
#      - 'REGION_SMALL_TEST=${_REGION_SMALL_TEST}'
#      - 'CLUSTER_NAME_SMALL_TEST=${_CLUSTER_NAME_SMALL_TEST}'
#      - 'PROJECT_NAME=${_PROJECT_NAME}'
#      - 'DATASET_NAME=${_DATASET_NAME}'
#      - 'TABLE_NAME_LARGE_TABLE=${_TABLE_NAME_LARGE_TABLE}'
#      - 'AGG_PROP_NAME_LARGE_TABLE=${_AGG_PROP_NAME_LARGE_TABLE}'

# 7. Start the unbounded source table e2e test
  - name: 'gcr.io/$PROJECT_ID/dataproc-flink-bigquery-connector-nightly'
    id: 'e2e-unbounded-read-test'
    waitFor: ['docker-build']
    entrypoint: 'bash'
    args: ['/workspace/cloudbuild/nightly.sh', 'e2e_unbounded_read_test']
    env:
      - 'GCS_JAR_LOCATION=${_GCS_JAR_LOCATION}'
      - 'PROJECT_ID=${_PROJECT_ID}'
      - 'REGION_UNBOUNDED_TABLE_TEST=${_REGION_UNBOUNDED_TABLE_TEST}'
      - 'CLUSTER_NAME_UNBOUNDED_TABLE_TEST=${_CLUSTER_NAME_UNBOUNDED_TABLE_TEST}'
      - 'PROJECT_NAME=${_PROJECT_NAME}'
      - 'DATASET_NAME=${_DATASET_NAME}'
      - 'TABLE_NAME_UNBOUNDED_TABLE=${_TABLE_NAME_UNBOUNDED_TABLE}'
      - 'AGG_PROP_NAME_UNBOUNDED_TABLE=${_AGG_PROP_NAME_UNBOUNDED_TABLE}'
      - 'PARTITION_DISCOVERY_INTERVAL=${_PARTITION_DISCOVERY_INTERVAL}'
      - 'TS_PROP_NAME=${_TS_PROP_NAME}'

# Maximum tolerance 60 minutes.
timeout: 3600s
logsBucket: '${_LOGS_BUCKET}'

options:
  machineType: 'N1_HIGHCPU_32'

